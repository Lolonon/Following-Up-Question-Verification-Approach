{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import product\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "filename = './GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "remove_list = ['do','Do','Does','does','did','Did',\n",
    "               'is','Is','was','Was','am','Am','Are','are','were','Were',\n",
    "               'What','what','How','how','When','when','Where','where','Who','who']\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "stop_words.extend(remove_list)\n",
    "\n",
    "nlp = spacy.load(\"en_coref_lg\")\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def following_up_question_chk(qn,qp):\n",
    "    #qn - question now\n",
    "    #qp - question past\n",
    "    verb_detection = False\n",
    "    coref_no_detection = True\n",
    "    text_connection = qp + ' ' + qn\n",
    "    doc = nlp(text_connection)\n",
    "    qn_tokens = word_tokenize(qn)\n",
    "    qn_tokens = [w for w in qn_tokens if not w in stop_words] \n",
    "    qp_tokens = word_tokenize(qp)\n",
    "    qp_tokens = [w for w in qp_tokens if not w in stop_words]\n",
    "    \n",
    "    qn_verb_detect_str = ' '.join(qn_tokens)\n",
    "    qp_verb_detect_str = ' '.join(qp_tokens)\n",
    "    \n",
    "    #print(qn_verb_detect_str)\n",
    "    #print(qp_verb_detect_str)\n",
    "    \n",
    "    qn_noun_list = []\n",
    "    qp_noun_list = []\n",
    "    \n",
    "    \n",
    "    qn_modified = qn\n",
    "    #check qn-question now has pronoun and possessive adjective references to qp\n",
    "    try:\n",
    "        if len(doc._.coref_clusters) >0:\n",
    "            coref_no_detection = False\n",
    "            print('Co-references detected')\n",
    "            for items in doc._.coref_clusters:\n",
    "                print(items.mentions)\n",
    "                print('Original question: ' + qn)\n",
    "                print('Are you asking: ' + qn.replace(str(items.mentions[1]),str(items.mentions[0])) )\n",
    "                qn_modified = qn_modified.replace(str(items.mentions[1]),str(items.mentions[0]))\n",
    "                qn_modified_tokens = qn_modified.split()\n",
    "                qn_modified_tokens = [word.lower() for word in qn_modified_tokens if word not in stop_words]\n",
    "            print('-'*30)\n",
    "    except:\n",
    "        print('Co-references are not detected')\n",
    "        print('-'*30)\n",
    "        qn_modified_tokens = qn_modified.split()\n",
    "        qn_modified_tokens = [word.lower() for word in qn_modified_tokens if word not in stop_words]\n",
    "        coref_no_detection = True\n",
    "        \n",
    "    #check any verb in qn-question now \n",
    "    for token in nlp(qn_verb_detect_str):\n",
    "        if token.pos_=='VERB':\n",
    "            verb_detection = True\n",
    "            print('A verb ' + str(token) + ' was detected')\n",
    "            \n",
    "    if verb_detection==False:\n",
    "        print('No verb was detected')\n",
    "    \n",
    "                                              \n",
    "    for token in nlp(qp):\n",
    "        if (token.pos_=='NOUN' or token.pos_=='PROPN'):\n",
    "            qp_noun_list.append(str(token))\n",
    "            \n",
    "    qp_noun_list = [x for x in qp_noun_list if x not in remove_list]\n",
    "    \n",
    "    for token in nlp(qn_modified):\n",
    "        if (token.pos_=='NOUN' or token.pos_=='PROPN'):\n",
    "            qn_noun_list.append(str(token))\n",
    "    qn_noun_list = [x for x in qn_noun_list if x not in remove_list]\n",
    "    \n",
    "    print('-'*30)        \n",
    "    print('Previous question noun is: ' + str(qp_noun_list))\n",
    "    print('Current question noun is: ' + str(qn_noun_list))\n",
    "    print('-'*30)\n",
    "    \n",
    "    qn_noun_str = ' '.join(qn_noun_list)\n",
    "    qp_noun_str = ' '.join(qp_noun_list)\n",
    "    \n",
    "    noun_group_similarity = nlp(qn_noun_str).similarity(nlp(qp_noun_str))\n",
    "    \n",
    "    '''\n",
    "    Google pretrained word2vec to get wmdDistance feature\n",
    "    '''\n",
    "    wmd = model.wmdistance(qp_tokens, qn_modified_tokens)\n",
    "    '''\n",
    "    Google pretrained word2vec most similar words\n",
    "    '''\n",
    "    words_pair_list = []\n",
    "    w2v_word_similarity_list = []\n",
    "    for word_1 , word_2 in product(qp_noun_list, qn_noun_list):\n",
    "        similarity = model.similarity(word_1,word_2)\n",
    "        w2v_word_similarity_list.append(similarity)\n",
    "        words_pair_list.append((word_1,word_2,similarity))\n",
    "    \n",
    "    try:\n",
    "        max_word_similarity = max(w2v_word_similarity_list)\n",
    "        ind = int(w2v_word_similarity_list.index(max_word_similarity))\n",
    "        word_pairs = words_pair_list[ind] \n",
    "        word_p = word_pairs[0]\n",
    "        word_n = word_pairs[1]\n",
    "        word_sim = word_pairs[2]\n",
    "\n",
    "        '''\n",
    "        Below is for synsets check, it is not necessary to enable\n",
    "        '''\n",
    "        #qn_noun_synsets = set(ss for word in qn_noun_list for ss in wordnet.synsets(word))\n",
    "        #qp_noun_synsets = set(ss for word in qp_noun_str for ss in wordnet.synsets(word))\n",
    "        #try:\n",
    "        #    best_synsets = max((wordnet.wup_similarity(s1, s2) or 0, s1, s2) for s1, s2 in product(qn_noun_synsets, qp_noun_synsets))\n",
    "        #except:\n",
    "        #    best_synsets = 0\n",
    "        #print(verb_detection)\n",
    "        #print(coref_no_detection)\n",
    "        #print(verb_detection and coref_no_detection)\n",
    "        if verb_detection and coref_no_detection:\n",
    "            print('It is not a following up question')\n",
    "            print('Two questions wmDistance is: ' + str(wmd))\n",
    "            print('NN similarity is: ' + str(noun_group_similarity))\n",
    "            print(word_p + ' and ' + word_n +' are the most relevent words \"(\"Based on Google w2v similarity which is: ' + str(word_sim) +\" )\")\n",
    "            #print('Bset NN Synset is: ' + str(best_synsets))\n",
    "            return False\n",
    "        else:\n",
    "            print('It is a following up question')\n",
    "            print('Two questions wmDistance is: ' + str(wmd))\n",
    "            print('NN similarity is: ' + str(noun_group_similarity))\n",
    "            print(word_p + ' and ' + word_n +' are the most relevent words \"(\"Based on Google w2v similarity which is: ' + str(word_sim) +\" )\")\n",
    "            #print('Bset NN Synset is: ' + str(best_synsets))\n",
    "            return True\n",
    "    except:\n",
    "        print('#'*30)\n",
    "        print('Empty noun list was detected')\n",
    "        print('#'*30)\n",
    "        if verb_detection and coref_no_detection:\n",
    "            print('It is not a following up question')\n",
    "            print('Two questions wmDistance is: ' + str(wmd))\n",
    "            print('NN similarity is: ' + str(noun_group_similarity))\n",
    "            #print('Bset NN Synset is: ' + str(best_synsets))\n",
    "            return False\n",
    "        else:\n",
    "            print('It is a following up question')\n",
    "            print('Two questions wmDistance is: ' + str(wmd))\n",
    "            print('NN similarity is: ' + str(noun_group_similarity))\n",
    "            #print('Bset NN Synset is: ' + str(best_synsets))\n",
    "            return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WMD_Distance Walkaround <=Google 300 is normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load Model\n",
    "#from gensim.models import KeyedVectors\n",
    "#filename = './GoogleNews-vectors-negative300.bin'\n",
    "#model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent_1 = 'Did you get all the information you wanted using the system?'\n",
    "#sent_2 = 'How easy was the system to obtain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent_1_token = sent_1.split()\n",
    "#sent_1_token = [w.lower() for w in sent_1_token if w not in stop_words]\n",
    "#sent_2_token = sent_2.split()\n",
    "#sent_2_token = [w.lower() for w in sent_2_token if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.963588026107289\n",
      "2.963588026107289\n"
     ]
    }
   ],
   "source": [
    "#distance_1_2 = model.wmdistance(sent_1_token, sent_2_token)\n",
    "#print(distance_1_2)\n",
    "#distance_2_1 = model.wmdistance(sent_2_token, sent_1_token)\n",
    "#print(distance_2_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16371556\n",
      "0.99999994\n"
     ]
    }
   ],
   "source": [
    "#for s1,s2 in product(['Information','System'],['System']):\n",
    "#    print(model.similarity(s1,s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7184451818466187\n"
     ]
    }
   ],
   "source": [
    "#check_1 = ['information', 'system']\n",
    "#check_2 = ['system']\n",
    "#distance = model.wmdistance(check_1, check_2)\n",
    "#print(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-references detected\n",
      "[the system, it]\n",
      "Original question: How easy was it to obtain\n",
      "Are you asking: How easy was the system to obtain\n",
      "------------------------------\n",
      "A verb obtain was detected\n",
      "------------------------------\n",
      "Previous question noun is: ['information', 'system']\n",
      "Current question noun is: ['system']\n",
      "------------------------------\n",
      "It is a following up question\n",
      "Two questions wmDistance is: 2.3765532010057937\n",
      "NN similarity is: 0.8528664493365101\n",
      "system and system are the most relevent words \"(\"Based on Google w2v similarity which is: 1.0 )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Captured correctly\n",
    "'''\n",
    "qp = 'Did you get all the information you wanted using the system?'\n",
    "qn = 'How easy was it to obtain'\n",
    "following_up_question_chk(qn,qp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-references are not detected\n",
      "------------------------------\n",
      "A verb live was detected\n",
      "------------------------------\n",
      "Previous question noun is: ['city', 'Finland']\n",
      "Current question noun is: ['people']\n",
      "------------------------------\n",
      "It is not a following up question\n",
      "Two questions wmDistance is: 3.2308872767218\n",
      "NN similarity is: 0.35229659112058176\n",
      "city and people are the most relevent words \"(\"Based on Google w2v similarity which is: 0.23614499 )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Captured wrongly\n",
    "'''\n",
    "qp = 'what is the biggest city in Finland?'\n",
    "qn = 'how many people live there??'\n",
    "following_up_question_chk(qn,qp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-references are not detected\n",
      "------------------------------\n",
      "A verb live was detected\n",
      "------------------------------\n",
      "Previous question noun is: ['information', 'system']\n",
      "Current question noun is: ['people']\n",
      "------------------------------\n",
      "It is not a following up question\n",
      "Two questions wmDistance is: 2.9295614928729514\n",
      "NN similarity is: 0.42065372089634406\n",
      "information and people are the most relevent words \"(\"Based on Google w2v similarity which is: 0.12545776 )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Captured correctly\n",
    "'''\n",
    "qp = 'Did you get all the information you wanted using the system?'\n",
    "qn = 'How many people live there'\n",
    "following_up_question_chk(qn,qp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-references detected\n",
      "[Glove Cleveland, he]\n",
      "Original question: how old was he?\n",
      "Are you asking: how old was Glove Cleveland?\n",
      "------------------------------\n",
      "No verb was detected\n",
      "------------------------------\n",
      "Previous question noun is: ['Glove', 'Cleveland']\n",
      "Current question noun is: ['Glove', 'Cleveland']\n",
      "------------------------------\n",
      "It is a following up question\n",
      "Two questions wmDistance is: 3.6464367404870766\n",
      "NN similarity is: 1.0\n",
      "Glove and Glove are the most relevent words \"(\"Based on Google w2v similarity which is: 1.0 )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Captured correctly\n",
    "'''\n",
    "qp = 'when did Glove Cleveland die?'\n",
    "qn = 'how old was he?'\n",
    "following_up_question_chk(qn,qp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-references detected\n",
      "[james, he]\n",
      "Original question: who did he marry?\n",
      "Are you asking: who did james marry?\n",
      "------------------------------\n",
      "A verb marry was detected\n",
      "------------------------------\n",
      "Previous question noun is: ['james']\n",
      "Current question noun is: ['james']\n",
      "------------------------------\n",
      "It is a following up question\n",
      "Two questions wmDistance is: 2.34818434715271\n",
      "NN similarity is: 1.0\n",
      "james and james are the most relevent words \"(\"Based on Google w2v similarity which is: 1.0 )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qp = 'where was james born?'\n",
    "qn = 'who did he marry?'\n",
    "following_up_question_chk(qn,qp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-references are not detected\n",
      "------------------------------\n",
      "A verb marry was detected\n",
      "------------------------------\n",
      "Previous question noun is: ['information', 'system']\n",
      "Current question noun is: []\n",
      "------------------------------\n",
      "##############################\n",
      "Empty noun list was detected\n",
      "##############################\n",
      "It is not a following up question\n",
      "Two questions wmDistance is: inf\n",
      "NN similarity is: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Captured correctly\n",
    "'''\n",
    "qp = 'Did you get all the information you wanted using the system?'\n",
    "qn = 'who did he marry?'\n",
    "following_up_question_chk(qn,qp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-references are not detected\n",
      "------------------------------\n",
      "No verb was detected\n",
      "------------------------------\n",
      "Previous question noun is: ['P', 'E', 'ratio', 'Apple']\n",
      "Current question noun is: ['Microsoft']\n",
      "------------------------------\n",
      "It is a following up question\n",
      "Two questions wmDistance is: inf\n",
      "NN similarity is: 0.3513736087337575\n",
      "Apple and Microsoft are the most relevent words \"(\"Based on Google w2v similarity which is: 0.52860063 )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Captured correctly\n",
    "'''\n",
    "qp = 'what is P/E ratio of Apple?'\n",
    "qn = 'how about Microsoft?'\n",
    "following_up_question_chk(qn,qp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-references are not detected\n",
      "------------------------------\n",
      "No verb was detected\n",
      "------------------------------\n",
      "Previous question noun is: ['P', 'E', 'ratio', 'Apple']\n",
      "Current question noun is: ['stock', 'price']\n",
      "------------------------------\n",
      "It is a following up question\n",
      "Two questions wmDistance is: 4.015905484418869\n",
      "NN similarity is: 0.3615854569650708\n",
      "ratio and stock are the most relevent words \"(\"Based on Google w2v similarity which is: 0.25689003 )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Captured correctly\n",
    "'''\n",
    "qp = 'what is P/E ratio of Apple?'\n",
    "qn = 'how about the stock price?'\n",
    "following_up_question_chk(qn,qp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc2 = nlp('My sister has a dog. She loves him')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc2._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[the system, it]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#doc = nlp('Did you get all the information you wanted using the system? How easy was it to obtain the information you want')\n",
    "\n",
    "#doc._.coref_clusters[0].mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9180178381713632"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#doc1 = nlp(\"Did you get all the information you wanted using system?\")\n",
    "#doc2 = nlp(\"How easy to obtain information you want\")\n",
    "#doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VERB'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#doc1[0].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the system: [the system, it]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#doc._.coref_clusters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV\n",
      "ADJ\n",
      "PART\n",
      "VERB\n",
      "NOUN\n",
      "PRON\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "#for token in doc2:\n",
    "#    print(token.pos_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
